{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Przeszukiwanie tekstu - algorytmy wyszukiwarek internetowych\n",
    "\n",
    "#### prezentuje Mateusz Puto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 1. Przeszukiwanie plików"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def bold(string):\n",
    "    return \"\\033[1m\" + string + \"\\033[0;0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— Dla pana — odpowiedział dowódca — jestem kapitanem Nemo; pan zaś i pańscy towarzysze jesteście dla mnie podróżnymi na „Nautilusie”.\n",
      "\n",
      "\u001b[1m564\u001b[0;0m\n",
      "\n",
      "\u001b[1mPo przeczytaniu książki pamiętamy to: \n",
      "\u001b[0;0m— Dla pana — odpowiedział dowódca — jestem kapitanem Nemo; pan zaś i pańscy towarzysze jesteście dla mnie podróżnymi na „Nautilusie”.\n",
      "\u001b[1m i to [spoiler !]:\n",
      "\u001b[0;0mSpodziewam się, że tak będzie. Mam nadzieję, że potężny „Nautilus” zwyciężył morze w najstraszniejszej jego otchłani, że przetrwał to, czego żaden okręt przetrwać nie zdołał. Jeśli tak jest, jeśli kapitan Nemo żegluje jeszcze po morzach, które mu są przybraną ojczyzną, oby uczucie zemsty wygasło w dzikim jego sercu! Oby ją wygoniło z jego myśli rozglądanie się w tylu cudach! Wykonawca sprawiedliwości niech ustąpi miejsca uczonemu dokonywającemu spokojnych poszukiwań podmorskich. Jeśli los jego jest niezwykły, to i wspaniały zarazem. Sam to osobiście pojąłem podczas dziesięciomiesięcznego, nadnaturalnego prawie mego istnienia.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"https://czytac.com/java-book/book/20_000_mil_podmorskiej_zeglugi\"\"\"\n",
    "\n",
    "# Wyszukiwanie samozakańczające\n",
    "dwa_tysiace_mil_podmorskiej_zeglugi = open('20_000_mil_podmorskiej_zeglugi.txt', 'r')\n",
    "\n",
    "for sentence in dwa_tysiace_mil_podmorskiej_zeglugi:\n",
    "    if \"Nautilus\" in sentence:\n",
    "        print(sentence)\n",
    "        break\n",
    "\n",
    "# Wyszukiwanie pełne\n",
    "dwa_tysiace_mil_podmorskiej_zeglugi = open('20_000_mil_podmorskiej_zeglugi.txt', 'r')\n",
    "\n",
    "nautilus_mentions = []\n",
    "for sentence in dwa_tysiace_mil_podmorskiej_zeglugi:\n",
    "    if \"Nautilus\" in sentence:\n",
    "        nautilus_mentions.append(sentence)\n",
    "\n",
    "print(bold(str(len(nautilus_mentions))), end=\"\\n\\n\")\n",
    "\n",
    "# Efekt niedawności (ang. \"recency effect\")\n",
    "print(bold(\"Po przeczytaniu książki pamiętamy to: \\n\") + nautilus_mentions[0] + bold(\" i to [spoiler !]:\\n\") + nautilus_mentions[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Miary jakości - dokładność, precyzja i czułość](https://www.mimuw.edu.pl/~wjaworski/SU/SU04_miary_jakosci.pdf)\n",
    "\n",
    "\n",
    "Macierz błędów (ang. \"Confusion matrix\"):\n",
    "\n",
    "                                                                  Przewidywanie\n",
    "\n",
    "                                                         Dopasowanie         Brak dopasowania\n",
    "\n",
    "                                ---------------------------------------------------------------------\n",
    "\n",
    "                                Dopasowanie             True Positive        False Positive\n",
    "        Stan rzeczywisty                         \n",
    "                                          \n",
    "                                Brak dopasowania        False Negative       True Negative\n",
    "\n",
    "<br><br>\n",
    "                        \n",
    "(1.1) Dokładność = (TP + TN) / (TP + TN + FP + FN) <---- Poprawna klasyfikacja\n",
    "\n",
    "(1.2) Precyzja = TP / (TP + FN)  <---------------------------------- Ilość poprawnie znalezionych wyników spośród tych wyświetlonych.\n",
    "\n",
    "(1.3) Czułość = TP / (TP + FP)   <----------------------------------- Ilość znalezionych wyników spośród wszystkich pasujących."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precyzja niepełnego wyszukania: 100.0% \n",
      "Czułość niepełnego wyszukania: 50.13%\n",
      "\n",
      "Precyzja niepoprawnego wyszukania: 51.07% \n",
      "Czułość niepoprawnego wyszukania: 100.0%\n"
     ]
    }
   ],
   "source": [
    "dwa_tysiace_mil_podmorskiej_zeglugi = open('20_000_mil_podmorskiej_zeglugi.txt', 'r')\n",
    "\n",
    "nemo = 0\n",
    "kapitan_nemo = 0\n",
    "nemo_or_ned = 0\n",
    "\n",
    "for sentence in dwa_tysiace_mil_podmorskiej_zeglugi:\n",
    "    # Poprawne wyszukanie\n",
    "    if \"Nemo\" in sentence:\n",
    "        nemo += 1\n",
    "    \n",
    "    # Wyszukanie, które pomija część wyników\n",
    "    if \"kapitan Nemo\" in sentence:\n",
    "        kapitan_nemo += 1\n",
    "\n",
    "    # Wyszukanie błędne\n",
    "    if \"Nemo\" in sentence or \"Ned\" in sentence:\n",
    "        nemo_or_ned += 1\n",
    "\n",
    "print(f\"Precyzja niepełnego wyszukania: {round(100 * kapitan_nemo / kapitan_nemo, 2)}% \\nCzułość niepełnego wyszukania: {round(100 * kapitan_nemo / nemo, 2)}%\")\n",
    "print()\n",
    "print(f\"Precyzja niepoprawnego wyszukania: {round(100 * nemo / nemo_or_ned, 2)}% \\nCzułość niepoprawnego wyszukania: {round(100 * nemo / nemo, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### algorytm Rabina-Karpa\n",
    "\n",
    "Wersja uproszczona bez rolowania hash'a, wyszukiwania wielu wzorców.\n",
    "\n",
    "[Rabin-Karp algorithm Wikipedia](https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search with Rabin-Karp: 0.11455478300194954\n"
     ]
    }
   ],
   "source": [
    "from timeit import Timer\n",
    "from functools import partial\n",
    "\n",
    "def rabinKarp(pattern, text):\n",
    "    \"\"\"Simplified Rabin-Karp algorithm\"\"\"\n",
    "    \n",
    "    m = len(pattern)\n",
    "    n = len(text)\n",
    "\n",
    "    hpattern = hash(pattern)\n",
    "\n",
    "    for i in range(0, n-m+1):\n",
    "        htext = hash(text[i : i+m])\n",
    "        \n",
    "        if htext == hpattern:\n",
    "            if text[i :i+m] == pattern:\n",
    "                return i\n",
    "    else:\n",
    "        return \"Not found\"\n",
    "\n",
    "\n",
    "\n",
    "text = open('20_000_mil_podmorskiej_zeglugi.txt', 'r').read()\n",
    "pattern = \"podług prawa natury wszystko idzie od prawej do lewej strony\"\n",
    "\n",
    "args = (pattern, text)\n",
    "t = Timer(partial(rabinKarp, *args))\n",
    "print(\"Search with Rabin-Karp:\", t.timeit(number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Zauważmy, że naiwne podejśce jest dla tego problemu znacząco szybsze. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive search: 0.09652286599884974\n"
     ]
    }
   ],
   "source": [
    "def naiveSearch(pattern, text):\n",
    "\n",
    "    m = len(pattern)\n",
    "    n = len(text)\n",
    "    \n",
    "    for i in range(0, n-m+1):\n",
    "        if text[i :i+m] == pattern:\n",
    "            return i\n",
    "    else:\n",
    "        return \"Not found\"\n",
    "\n",
    "text = open('20_000_mil_podmorskiej_zeglugi.txt', 'r').read()\n",
    "pattern = \"podług prawa natury wszystko idzie od prawej do lewej strony\"\n",
    "\n",
    "args = (pattern, text)\n",
    "t = Timer(partial(naiveSearch, *args))\n",
    "print(\"Naive search:\", t.timeit(number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Jednak generalna koncepcja algorytmu może być zastosowana do stworzenia systemu wyszukiwania informacji. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def precomputed_hashes(hpattern, hashes):        \n",
    "    return hpattern in hashes\n",
    "\n",
    "\n",
    "text = open('bible.txt', 'r')\n",
    "\n",
    "hashes = dict()\n",
    "verse = \"\"\n",
    "for paragraph in text:\n",
    "    if paragraph.strip() == \"\":\n",
    "        verse = verse.strip()\n",
    "\n",
    "        b = bytes(verse, encoding='utf-8')\n",
    "        hash_val = hashlib.sha256(b).hexdigest()\n",
    "        bucket = int(hash_val, 16) % 100\n",
    "\n",
    "        if bucket in hashes.keys():\n",
    "            hashes[bucket].append(hash_val)\n",
    "        else:\n",
    "            hashes[bucket] = list()\n",
    "            hashes[bucket].append(hash_val) \n",
    "\n",
    "        verse = \"\"\n",
    "    else:\n",
    "        verse += \" \" + paragraph.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple patterns hashed search: 0.0002994339956785552\n"
     ]
    }
   ],
   "source": [
    "not_in_text_patterns = [\"Oczekiwałem na kapitana Nemo, ale się wcale nie pokazał\",\n",
    "\"Zakomunikowałem te spostrzeżenia i obawy kapitanowi Nemo\",\n",
    "\"Poczekawszy jeszcze trochę, udałem się do salonu\",\n",
    "\"Śród takich okoliczności płynęliśmy aż do 13 marca\",\n",
    "\"Zakładanie dokonywało się pomyślnie\",]\n",
    "\n",
    "bible_patterns = [\"1:3 And God said, Let there be light: and there was light.\", \n",
    "\"4:13 Therefore set I in the lower places behind the wall, and on the higher places, I even set the people after their families with their swords, their spears, and their bows.\",\n",
    "\"37:11 But the meek shall inherit the earth; and shall delight themselves in the abundance of peace.\",\n",
    "\"16:6 We have heard of the pride of Moab; he is very proud: even of his haughtiness, and his pride, and his wrath: but his lies shall not be so.\",\n",
    "\"5:8 And there are three that bear witness in earth, the Spirit, and the water, and the blood: and these three agree in one.\"]\n",
    "\n",
    "patterns = bible_patterns + not_in_text_patterns\n",
    "\n",
    "def findPatterns(patterns, hashes):\n",
    "    hpatterns = [hashlib.sha256(bytes(pattern, encoding='utf-8')).hexdigest() for pattern in patterns]\n",
    "\n",
    "    for hpattern in hpatterns:\n",
    "        bucket = int(hpattern, 16) % 100\n",
    "\n",
    "        if hpattern in hashes[bucket]:\n",
    "            pass\n",
    "            #print(\"Match\")\n",
    "        else:\n",
    "            pass\n",
    "            #print(\"Miss\")\n",
    "\n",
    "args = (patterns, hashes)\n",
    "t = Timer(partial(findPatterns, *args))\n",
    "print(\"Multiple patterns hashed search:\", t.timeit(number=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive search with multiple patterns: 0.06325988099706592\n"
     ]
    }
   ],
   "source": [
    "text = open('bible.txt', 'r')\n",
    "\n",
    "def naiveWholeSents(patterns, text):\n",
    "    verse = \"\"\n",
    "    for paragraph in text:\n",
    "        if paragraph.strip() == \"\":\n",
    "            verse = verse.strip()\n",
    "\n",
    "            if verse in patterns:\n",
    "                #print(\"Match\")\n",
    "                pass\n",
    "            \n",
    "            verse = \"\"\n",
    "        else:\n",
    "            verse += \" \" + paragraph.strip()\n",
    "        \n",
    "\n",
    "args = (patterns, text)\n",
    "t = Timer(partial(naiveWholeSents, *args))\n",
    "print(\"Naive search with multiple patterns:\", t.timeit(number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Wyszukiwanie pełnotekstowe\n",
    "\n",
    "- wykorzystywanie tylko tytułów i meta-tagów\n",
    "- spamdexowanie\n",
    "- Która z wyszukiwarek internetowych była jako pierwsza wyszukiwarką pełnotekstową?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"altavista.png\" alt=\"AltaVista search engine\" width=\"1000\"/>\n",
    "\n",
    "[ źródło: [https://digital.com/wp-content/uploads/4307188078_f290aecf49_o.png](https://digital.com/wp-content/uploads/4307188078_f290aecf49_o.png)]\n",
    "\n",
    "_______________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Indeksy w książkach:\n",
    "\n",
    "<img src=\"book-index.jpg\" alt=\"Indexes in books\" />\n",
    "\n",
    "[ źródło: [https://book-editing.com/why-book-indexing/](https://www.book-editing.com/why-book-indexing/) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Odwrócony indeks\n",
    "\n",
    "- budowanie indeksu\n",
    "- sortowanie\n",
    "- wyszukiwanie\n",
    "- dodatkowe informacje: częstość, miejsce wystąpienia\n",
    "- kompresja\n",
    "\n",
    "<img src=\"inverted-index.png\" width=400 />\n",
    "\n",
    "[ źródło: https://nlp.stanford.edu/IR-book/html/htmledition/a-first-take-at-building-an-inverted-index-1.html ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wyszukiwanie pełnotekstowe może służyć poprawie nawigacji na stronie. Gdy widzimy okienko wyszukiwania na stronie jaka technologia zasila to rozwiązanie?\n",
    "\n",
    "\n",
    "- Apache Lucene i rozwiązania oparte o Lucene takie jak np. Solr- otwartoźródłowa biblioteka, najbardziej modyfikowalna opcja\n",
    "\n",
    "- Google Programmable Search Engine, AWS CloudSearch, Elasticsearch , Algolia itp. - a więc rozwiązania od różnych dostawców takich usług\n",
    "\n",
    "- Wyszukiwanie bezpośrednio w bazie danych lub rozwiązanie typu client-side (Czy naprawdę tego chcemy?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "----------------------------\n",
    "* [How to add full text search to your website by Sam Dutton](https://medium.com/dev-channel/how-to-add-full-text-search-to-your-website-4e9c80ce2bf4)\n",
    "<br><br>\n",
    "* [Apache Lucene](https://lucene.apache.org/)\n",
    "* [Lucene talk with tamar Syn-Hershko](https://www.youtube.com/watch?v=Nf9p-d01p78)\n",
    "<br><br>\n",
    "* [Google Programmable Search Engine](https://developers.google.com/custom-search)\n",
    "* [AWS CloudSearch](https://aws.amazon.com/cloudsearch/)\n",
    "* [Elasticsearch](https://www.elastic.co/)\n",
    "<br><br>\n",
    "* [Algolia vs Elasticsearch according to Algolia Part 1](https://www.algolia.com/blog/engineering/algolia-v-elasticsearch-latency/)\n",
    "* [Algolia vs Elasticsearch according to Algolia Part 2](https://www.algolia.com/blog/engineering/algolia-v-elasticsearch-relevance/)\n",
    "* [Inside the Algolia Engine Blog Series](https://www.algolia.com/blog/engineering/inside-the-algolia-engine-part-1-indexing-vs-search/)\n",
    "* [Algolia ranking algorithm](https://www.algolia.com/blog/how-algolia-tackled-the-relevance-problem-of-search-engines/)\n",
    "* [Algolia AI: vectors vs hashes](https://www.algolia.com/blog/ai/vectors-vs-hashes/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### przykład Algolii\n",
    "\n",
    "<img src=\"algolia.svg\" width=500/>\n",
    "\n",
    "* Wyszukiwanie przyrostowe - wykorzystanie drzewa trie\n",
    "* Poprawianie błędów - dystans edycyjny Damerau-Levenshtein'a\n",
    "* Podkreślanie znalezionych słów kluczowych\n",
    "* Lematyzacja i rozszerzanie zapytania\n",
    "* Tworzenie rankingu - algorytm tie-breaking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Wyszukiwanie informacji (IR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Rozszerzanie zapytania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meaning: a motor vehicle with four wheels; usually propelled by an internal combustion engine,\n",
      " lemmas: [Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'), Lemma('car.n.01.machine'), Lemma('car.n.01.motorcar')]\n",
      "\n",
      "meaning: a wheeled vehicle adapted to the rails of railroad,\n",
      " lemmas: [Lemma('car.n.02.car'), Lemma('car.n.02.railcar'), Lemma('car.n.02.railway_car'), Lemma('car.n.02.railroad_car')]\n",
      "\n",
      "meaning: the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant,\n",
      " lemmas: [Lemma('car.n.03.car'), Lemma('car.n.03.gondola')]\n",
      "\n",
      "meaning: where passengers ride up and down,\n",
      " lemmas: [Lemma('car.n.04.car'), Lemma('car.n.04.elevator_car')]\n",
      "\n",
      "meaning: a conveyance for passengers or freight on a cable railway,\n",
      " lemmas: [Lemma('cable_car.n.01.cable_car'), Lemma('cable_car.n.01.car')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "for syn in wordnet.synsets(\"car\"):\n",
    "    print(f\"meaning: {syn.definition()},\\n lemmas: {syn.lemmas()}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'car auto automobile machine motorcar'\n"
     ]
    }
   ],
   "source": [
    "def query_expansion(word, meaning=0):\n",
    "    query = ''\n",
    "    lemmas = wordnet.synsets(word)[meaning].lemmas()\n",
    "    for lemma in lemmas:\n",
    "\n",
    "        query += \" \" + lemma.name()\n",
    "\n",
    "    return query.strip()\n",
    "\n",
    "\n",
    "new_query = query_expansion(\"car\")\n",
    "print(f\"Query: '{new_query}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tokenizery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', '(', 'York', ')', '.']\n",
      "['Please', '(', 'buy', ')', 'me', 'two', 'of', 'them', '.']\n",
      "['(', 'Thanks', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, WhitespaceTokenizer\n",
    "\n",
    "sents = '''Good muffins cost $3.88\\nin New (York).  Please (buy) me\\ntwo of them.\\n(Thanks).'''\n",
    "\n",
    "tokens = []\n",
    "for sent in sent_tokenize(sents):\n",
    "    tokens.append(word_tokenize(sent))\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'i', 'onli', 'could', ',', 'i', \"'d\", 'be', 'run', 'up', 'that', 'hill']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "\n",
    "sent = '''If I only could, I'd be running up that hill'''\n",
    "\n",
    "tokens = []\n",
    "for token in word_tokenize(sent):\n",
    "    tokens.append(token)\n",
    "\n",
    "print([snowball.stem(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Model boolowski\n",
    "\n",
    "* Zazwyczaj każde słowo jest interpretowane jako oddzielny token.\n",
    "* Wyszukiwane wyrażenia mogą być połączone operatorami logicznymi AND, OR, a nawet NEAR.\n",
    "* Konieczne stworzenie odwróconego indeksu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "from unittest import result\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class FullTextSearch:\n",
    "    \"\"\"Full-text search minimal example\"\"\"\n",
    "    def __init__(self):\n",
    "        self.inverted_index = {}\n",
    "        self.stop_words = stopwords.words()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.documents = {}\n",
    "\n",
    "\n",
    "    def search(self, query):\n",
    "        tokens = self.tokenize(query)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for token in tokens:\n",
    "            matches = set()\n",
    "\n",
    "            if token in self.inverted_index:\n",
    "                for document in self.inverted_index[token]:\n",
    "                    matches.add(document)\n",
    "            \n",
    "            results.append(matches)\n",
    "\n",
    "        return reduce(lambda x, y: x & y, results)\n",
    "\n",
    "    def add_document(self, doc, id):\n",
    "        self.documents[id]  = doc\n",
    "        self.index(doc, id)\n",
    "\n",
    "    def tokenize(self, query):\n",
    "        return word_tokenize(query)\n",
    "\n",
    "    def index(self, doc, id):\n",
    "        indexes = set()\n",
    "\n",
    "        for word in self.tokenize(doc):\n",
    "            if word not in self.stop_words:\n",
    "                indexes.add(self.stemmer.stem(word))\n",
    "\n",
    "        for word in indexes:\n",
    "            if not word in self.inverted_index:\n",
    "                self.inverted_index[word] = list()\n",
    "            self.inverted_index[word].append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "file_ids = movie_reviews.fileids()\n",
    "print(len(file_ids))\n",
    "\n",
    "fts_example = FullTextSearch()\n",
    "\n",
    "for id in file_ids[:100]:\n",
    "    fts_example.add_document(movie_reviews.raw(id), id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg/cv000_29416.txt'}\n",
      "{'neg/cv013_10494.txt', 'neg/cv026_29229.txt', 'neg/cv000_29416.txt'}\n"
     ]
    }
   ],
   "source": [
    "problem_review = fts_example.search(\"problem 4/10\")\n",
    "cool_kid_review = fts_example.search(\"cool kid\")\n",
    "\n",
    "print(problem_review, end=\"\\n\")\n",
    "print(cool_kid_review, end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Modele algebraiczne\n",
    "\n",
    "- bag-of-words, zlicza ilość wystąpień (ang. \"term frequency\")\n",
    "- częstość w dokumentach (ang. \"document frequency\")\n",
    "- idf (ang. \"inverse document frequency\")\n",
    "\n",
    "<img src=\"tf-idf.png\" />\n",
    "\n",
    "[ źródło: [https://pl.wikipedia.org/wiki/TFIDF](https://pl.wikipedia.org/wiki/TFIDF) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wykorzystanie **tf-idf**:\n",
    "\n",
    " - Model przestrzeni wektorowej\n",
    "\n",
    "<img src=\"vector-space.png\" />\n",
    "\n",
    "[ źródło: [https://www.researchgate.net/figure/A-query-and-document-representation-in-the-vector-space-model_fig1_220692395](https://www.researchgate.net/figure/A-query-and-document-representation-in-the-vector-space-model_fig1_220692395) ]\n",
    "\n",
    "Jak mierzyć podobieństwo między dokumentami i zapytaniami?\n",
    "\n",
    "- kosinus między wektorami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Modele probabilistyczne\n",
    "\n",
    "- Probabilistyczna zasada szeregowania mówi, że dokumenty powinny być uszeregowane w kolejności prawdopodobieństwa ich użyteczności lub dopasowania. [ https://www.emerald.com/insight/content/doi/10.1108/eb026647/full/html?skipTracking=true ]\n",
    "\n",
    "- Działają w oparciu o zasadę Bayesa (choć nie tylko: https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_People/Profs/goran/5-Probabilistic-Retrieval-FSS20.pdf)\n",
    "\n",
    "<img src=\"bayes.png\" />\n",
    "\n",
    "[ źródło: https://medium.com/@msalmon00/bayes-rule-and-cookies-a-primer-in-code-3ee15b6dc755 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- założenie Binary Independence Model, nie występują zależności między elementami. Możemy traktować każdą część wektora jako niezależną jednostkę.\n",
    "\n",
    "- Najszerzej wykorzystywanym modelem probabilistycznym jest __BM25__.\n",
    "\n",
    "<img src=\"bm25.png\" />\n",
    "\n",
    "[ źródło: [https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_People/Profs/goran/5-Probabilistic-Retrieval-FSS20.pdf](https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_People/Profs/goran/5-Probabilistic-Retrieval-FSS20.pdf) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Relevance feedback\n",
    "\n",
    "<img src=\"relevance-feedback.jpg\" width=400 />\n",
    "\n",
    "[ źródło: (https://studfile.net/html/2706/143/html_oTTGrBg9KM._waU/htmlconvd-t8XooN216x1.jpg)[https://studfile.net/html/2706/143/html_oTTGrBg9KM._waU/htmlconvd-t8XooN216x1.jpg] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- algorytm Rocchia\n",
    "- strona UX oceniania jakości wyników\n",
    "- clickthrough data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Modele językowe\n",
    "\n",
    "- Model językowy generuje rozkład prawdopodobieństwa nad słowami ze słownika. \n",
    "- Może być zrealizowany jako automat skończony, z którego możemy odczytać prawdopodobieństwa sekwencji. \n",
    "- Wygenerowany dla konkretnego dokumentu model językowy jest używany do określenia prawdopodobieństwa warunkowego na otrzymanie zapytania pod warunkiem, że zaszedł warunek wystąpienia modela językowego dla dokumentu. \n",
    "\n",
    "\n",
    "<img src=\"language-model.png\" width=500/>\n",
    "[ źródło: https://towardsdatascience.com/the-beginners-guide-to-language-models-aa47165b57f9 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Wyszukiwarki internetowe\n",
    "\n",
    "Istnieją różne rozwiązania pozwalające na wyszukiwanie w sieci, wśród nich: rozwiązanie Microsoftu Bing – wbudowane w wyszukiwarkę Edge, Yahoo – posiadające kilkunastoprocentowy udział w rynku wyszukiwarek w Japonii, czy Baidu ze swoim dominującym udziałem w Chinach . Jednak niekwestionowanym liderem na globalnym rynku wyszukiwania internetowego pozostaje Google ze swoim ponad 90% udziałem.\n",
    "\n",
    "<img src=\"yahoo.png\" width=300/>\n",
    "<img src=\"bing.png\" width=300/>\n",
    "<img src=\"duckduckgo.png\" width=300/>\n",
    "<img src=\"baidu.png\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Jak działa Google według Google'a?\n",
    "\n",
    "- [How search works](https://www.google.com/search/howsearchworks/)\n",
    "- [\"Szczegółowy\" przewodnik po działaniu wyszukiwarki Google](https://developers.google.com/search/docs/fundamentals/how-search-works)\n",
    "- [Search On 2022](https://blog.google/products/search/search-on-2022-announcements/)\n",
    "\n",
    "Jak naprawdę działa Google? Najlepiej sprawdzić to samemu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.brainkart.com/article/Trends-in-Information-Retrieval_11614/\n",
      "https://credencepressltd.com/journal/uploads/archive/202015998633983597529394.pdf\n",
      "https://misc.library.ucsb.edu/untangle/lager.html\n",
      "https://www.researchgate.net/publication/318530538_Trends_and_Issues_in_Modern_Information_Retrieval\n",
      "https://www.igi-global.com/chapter/information-retrieval-models/198585\n",
      "https://www.nowpublishers.com/INR\n",
      "https://www.academia.edu/33978409/_Trends_and_issues_in_Modern_Information_Retrieval_\n",
      "https://link.springer.com/chapter/10.1007/978-981-15-5554-1_14\n",
      "https://www.slideshare.net/abhay.ratnaparkhi/latest-trends-inaiandinformationretrieval-autosaved\n",
      "https://ieeexplore.ieee.org/document/8759000\n",
      "https://pubmed.ncbi.nlm.nih.gov/5515231/\n"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "\n",
    "query = 'new trends in information retrieval'\n",
    "for url in search(query):\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pająki (crawlery) internetowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Kryteria:\n",
    "    \n",
    "- Pokrycie (ang. \"coverage\"), czyli udział witryn odwiedzonych wśród wszystkich witryn\n",
    "- Świeżość (ang. \"freshness\"), czyli miara mówiąca nam jak dawno witryny były odwiedzane\n",
    "- Pełność (ang. \"completeness\"), czyli jaka część zasobów z danej strony została przetworzona i zindeksowana\n",
    "\n",
    "Dodatkowo pająk internetowy powinien cechować się uprzejmością. Uprzejmy robot powinien nie nadwyrężać zasobów strony i nie wysyłać wszystkich żądań o zasoby w jednym momencie. Pająk powinien respektować ograniczenia na niego nałożone, które są umieszczone w pliku o nazwie 'robots.txt'.\n",
    "\n",
    "https://www.google.com/robots.txt\n",
    "\n",
    "\n",
    "<img src=\"robots.png\" alt=\"przykładowy plik robots.txt\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frontier size after 2 iterations of crawling: 105\n"
     ]
    }
   ],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "# https://en.wikipedia.org/wiki/List_of_HTTP_header_fields#Request_fields\n",
    "headers = {\n",
    "    'User-Agent': 'My tiny bot 0.1',\n",
    "    'From': 'mateuszmputo@gmail.com'\n",
    "}\n",
    "\n",
    "seed = ['https://mateuszputo.github.io/contact.html']\n",
    "\n",
    "frontier = seed\n",
    "iterations = 2\n",
    "for i in range(0, iterations):\n",
    "    page = requests.get(frontier[0], headers=headers)\n",
    "    webpage = html.fromstring(page.content)\n",
    "\n",
    "    urls = webpage.xpath('//a/@href')\n",
    "    [frontier.append(url) for url in urls]\n",
    "    frontier.pop(0)\n",
    "\n",
    "print(f\"Frontier size after {iterations} iterations of crawling: {len(frontier)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Crawling stron typu RIA\n",
    "\n",
    "Wraz z wprowadzeniem do użytku stron, które w swojej konstrukcji nie opierają się na hiperłączac między podstronami, lecz generują DOM strony dynamicznie przy użyciu skryptów JavaScript, ciężko mówić o pająkach internetowych nie wspominając o Deep-Web crawlingu.\n",
    "\n",
    "- Jak przejść po stronie która zmienia się dynamicznie na podstawie akcji użytkownika?\n",
    "\n",
    "Nowoczesny pająk uruchamia taką Rich Internet Application na silniku JavaScript, sprawdza czy widział daną stronę za pomocą modułu DOM-Seen i jeśli nie to przeprowadza ekstrakcję wszystkich zdarzeń do uruchomienia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Pytanie: Czym jest większość zdarzeń JavaScript, które próbujemy uruchomić?\n",
    "\n",
    "<img src=\"deepweb-crawler.png\" />\n",
    "\n",
    "[ źródło: [https://link.springer.com/article/10.1007/s11280-018-0602-1](https://link.springer.com/article/10.1007/s11280-018-0602-1) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Zaawansowane Googlowanie:\n",
    "\n",
    "- Wyszukiwanie frazy\n",
    "- Użycie operatorów logicznych AND, OR\n",
    "- Wyszukiwanie na danej stronie \"site:\"\n",
    "- Wersja cache strony \"cache:\"\n",
    "- wyłączenia ze znakiem minus\n",
    "- Filtrowanie wyników\n",
    "\n",
    "... żeby zostać 🥷🥷🥷 googlowania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### PageRank\n",
    "\n",
    "Było na zajęciach - nie ma sensu przytaczać.\n",
    "\n",
    "Model PageRank można interpretować jako modelujący zachowanie losowego błądzenia po stronach przez przeciętnego użytkownika.\n",
    "\n",
    "<img src=\"surfer.jpg\" width=\"400\"/>\n",
    "\n",
    "[ źródło: [https://www.surfertoday.com/surfing/how-many-surfers-are-there-in-the-world](https://www.surfertoday.com/surfing/how-many-surfers-are-there-in-the-world)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Z ciekawostek: \n",
    "\n",
    "- PageRank był powodem stworzenia wyszukiwarki Google.\n",
    "- Lepszy predyktor liczby cytowań od samej znanej liczby cytowań.\n",
    "\n",
    "[ źródło: [The PageRank Citation Ranking: Bringing Order to the Web](http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### BigTable\n",
    "\n",
    "- NoSQL-owa baza danych. \n",
    "- Ma postać wielowymiarowej posortowanej tablicy asocjacyjnej (mapy).\n",
    "- Indeksowana w oparciu o klucz rzędu i kolumny, zapisywany znacznik czasu.\n",
    "- Dane przechowywane jako nieinterpretowalny ciąg bajtów.\n",
    "- Pozwala na przechowywanie wielu wersji pliku w jednym rekordzie.\n",
    "- Kompresja osiągająca wynik 10 do 1.\n",
    "\n",
    "<img src=\"bigtable-architecture.svg\" width=500/>\n",
    "\n",
    "[ źródło: [https://cloud.google.com/bigtable/img/bigtable-architecture.svg](https://cloud.google.com/bigtable/img/bigtable-architecture.svg) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Wprowadzone zmiany\n",
    "\n",
    "- Graf wiedzy Google\n",
    "- opcja filtrowania wyników\n",
    "- obsługa wielu języków\n",
    "- aktualizacja zapewniająca świeżość \"caffeine\"\n",
    "- wyszukiwanie mową oraz obrazem\n",
    "- i wiele innych: https://en.wikipedia.org/wiki/Timeline_of_Google_Search\n",
    "\n",
    "<img src=\"isaac-newton.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Wykorzystanie uczenia maszynowego\n",
    "\n",
    "Wyszukiwanie internetowe wykorzystuje duże zasoby danych. Nie jest więc dziwnym, że wiele z problemów wyszukiwania można usprawnić za pomocą rozwiązań uczenia maszynowego i uczenia głębokiego. To, co w Algolii było rozwiązywane za pomocą drzewa, czyli poprawianie literówek, Google rozwiązuje za pomocą sieci neuronowej składającej się z 680 milionów parametrów, którą udaje im się uruchomić w dwie milisekundy\n",
    "\n",
    "- RankBrain\n",
    "\n",
    "Pierwszym z systemów głębokiego uczenia wykorzystywanym w Googlu był RankBrain. Pomaga on w łączeniu słów i konceptów. Jak podaje Google: gdy wpiszemy zapytanie pytające o \"nazwę konsumenta będącego na szczycie łańcucha pokarmowego\" to algorytm RankBrain pomoże w odnalezieniu wyników zawierających wspomnienie o \"drapieżniku szczytowym\".\n",
    "\n",
    "Co ciekawe gdy sprawdzimy to hasło, wyszukiwanie działa ale na razie tylko w języku angielskim.\n",
    "\n",
    "- neural matching\n",
    "\n",
    "Kolejnym ważnym rozwiązaniem było rozwiązanie \"neural matching\" ulepszające łączenie ze sobą zapytań ze stronami [42]. Działa ono prawdopodobnie wykorzystując rozszerzanie zapytania i/lub dokumentu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- BERT\n",
    "\n",
    "W 2019 powstał wielki model językowy BERT, który umożliwia statystyczne rozumienie tekstu . Umożliwia on prawdopodobnie ponowne szeregowanie odnalezionych wyników (ang. \"re-ranking\"), a wg. Googla pozwala też na niepomijanie ważnych słow, które inaczej mogłyby być uznane za słowa ze stop listy (ang. \"stop words\").\n",
    "\n",
    "- MUM\n",
    "\n",
    "Jednak największym do tej pory przedsięwzięciem jest MUM czyli \"Multitask Unified Model\" pozwalający na rozumienie i generowanie tekstu w 75 językach oraz rozwiązujący wiele rodzajów zadań. Model ten jest również multi modalny, może zrozumieć oprócz tekstu również obraz. Model ma być początkowo zintegrowany z Google Lens, wyszukiwarką obrazem w telefonie.\n",
    "\n",
    "- [How AI powers great search results](https://blog.google/products/search/how-ai-powers-great-search-results/)\n",
    "- [The ABCs of spelling in Google Search](https://blog.google/products/search/abcs-spelling-google-search/)\n",
    "- [MUM: A new AI milestone for understanding information](https://blog.google/products/search/introducing-mum/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 5. Przetwarzanie języka naturalnego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Książka do bibilioteki NLP w Pythonie:\n",
    "\n",
    "    - [NLTK Book](https://www.nltk.org/book/)\n",
    "\n",
    "<img src=\"nltk.jpg\" width=300 />\n",
    "\n",
    "[ źródło: [https://static01.helion.com.pl/global/okladki/326x466/e_2gte.jpg](https://static01.helion.com.pl/global/okladki/326x466/e_2gte.jpg) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Wyszukiwanie informacji używa klasycznie statystycznych miar językowych do określania charakterystyk dokumentów. Dziedzina przetwarzania języka naturalnego (NLP) zajmuje się wykonywaniem manipulacji na tekście naturalnym, czyli takim, jakim posługują się ludzie na co dzień w mowie i piśmie, przy użyciu komputerów. Od prostego zliczania wystąpień i innych miar statystycznych po zrozumienie tekstu i generowanie odpowiedzi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Hipoteza dystrybucyjna\n",
    "\n",
    "Słowa o podobnym znaczeniu występują w podobnych kontekstach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Dystans edycyjny\n",
    "\n",
    "Dystans edycyjny to miara podobieństwa dwóch łańcuchów tekstowych, która jest mierzona za pomocą liczby podstawowych operacji, które należy wykonać, aby zamienić jeden z łańcuchów na drugi. Jednym z popularniejszych metryk jest odległość Levenshteina pozwalająca na operację usunięcia, dodania i zamiany znaku w łańcuchu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. Zastosowanie metod uczenia maszynowego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Zadanie wyszukiwania informacji ad-hoc polega na znalezieniu rankingu najbardziej podobnego do optymalnego opartego o prawdopodobieństwo dopasowania. Funkcja szeregowania Q × D → R oblicza wynik dopasowania dla pary dokument, zapytanie. Funkcja taka może być skonstruowana w ten sposób, że najpierw oblicza reprezentacje dla dokumentów, zapytań, par dokumentów i zapytań, a na końcu agreguje te reprezentacje. Takie systemy są systemami reprezentacyjnymi, w przeciwieństwie do systemów interakcyjnych, polegających na aktywnym przetworzeniu zapytania wraz z dokumentem, aby otrzymać wynik dopasowania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Word2vec, doc2vec\n",
    "\n",
    "- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "- [Distributed Representations of Sentences and Documents](https://arxiv.org/pdf/1405.4053.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec vector operations 'paris - france + italy':\n",
      "[('italy', 0.8724863529205322), ('brazil', 0.823341429233551), ('regina', 0.764968752861023), ('thailand', 0.7616036534309387), ('greet', 0.7495027184486389), ('perform', 0.743165910243988), ('demi', 0.7425128221511841), ('roma', 0.7410842180252075), ('milan', 0.7396031618118286), ('band', 0.7381407022476196)]\n",
      "Basic usage 'similar to word %pc%':\n",
      "[('tablet', 0.8791574835777283), ('iphone', 0.8717852830886841), ('windows', 0.867980420589447), ('ipod', 0.8656975030899048), ('internet', 0.8508771061897278), ('notebook', 0.8482425212860107), ('flash', 0.8476564884185791), ('ipad', 0.8319953083992004), ('auto', 0.8283662796020508), ('app', 0.826338529586792)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
    "\n",
    "rome_vec = glove_vectors.get_vector(\"paris\", norm=False) - glove_vectors.get_vector(\"france\", norm=False) + glove_vectors.get_vector(\"italy\", norm=False)\n",
    "\n",
    "print(\"Word2vec vector operations 'paris - france + italy':\")\n",
    "print(glove_vectors.most_similar(rome_vec))\n",
    "\n",
    "print(\"Basic usage 'similar to word %pc%':\")\n",
    "print(glove_vectors.most_similar(\"pc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Transformery\n",
    "\n",
    "Czy chodzi o coś takiego?\n",
    "\n",
    "<img src=\"transformers.jpg\" />\n",
    "\n",
    "[ źródło: [https://netflix.com](https://netflix.com)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Transformery\n",
    "\n",
    "Raczej o coś takiego:\n",
    "\n",
    "<img src=\"transformer.png\" />\n",
    "\n",
    "[ źródło: [https://i.stack.imgur.com/eAKQu.png](https://i.stack.imgur.com/eAKQu.png)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### MS MARCO\n",
    "\n",
    "- Jeden z najpopularniejszych zbiorów danych dla trenowania i testowania modeli wyszukiwania informacji.\n",
    "- Składa się on z ponad miliona zapytań pozyskanych z logów wyszukiwarki Bing, każde z odpowiedzią wygenerowaną przez człowieka.\n",
    "- Ponadto zbiór ten zawiera prawie 9 milionów paragrafów pozyskanych z ponad 3,5 miliona dokumentów. Ustępy są wybrane jako 10 odpowiedzi na pytanie ustalonych przy pomocy wyszukiwarki Bing.\n",
    "- Kilka ustandaryzowanych zadań w tym ranking dokumentów oraz ranking paragrafów.\n",
    "- Najlepsze wyniki osiągnięte dla zadań dostępne na stronie głównej projektu.\n",
    "\n",
    "<img src=\"msmarco.png\" width=500/>\n",
    "\n",
    "[ https://microsoft.github.io/msmarco/ ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Modele\n",
    "\n",
    "- BM25 + BERT, osiągnięto poprawę 27% na miarze MRR@10.\n",
    "- Doc2query, rozszerzenie dokumentu o 10 pytań (top-k sampling)\n",
    "- ColBert (z faiss), dzięki architekturze bienkodera osiąga lepszą złożoność czasową.  \n",
    "- HLART, efektywny trzeci stopień wyszukiwania\n",
    "- PROP, dostosowany trening na modelowaniu językowym [2021 SOTA]\n",
    "- Condenser i coCondenser [SOTA 2022]\n",
    "    * pretrening\n",
    "    * uniknięcie rozproszonej reprezentacji\n",
    "    * dotrenowanie na zadaniu określania odległości między dokumentami\n",
    "\n",
    "Prace naukowe:\n",
    "- [Passage re-ranking with BERT](https://arxiv.org/pdf/1901.04085.pdf)\n",
    "- [Document Expansion by Query Prediction](https://arxiv.org/pdf/1904.08375.pdf)\n",
    "- [ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://arxiv.org/pdf/2004.12832.pdf)\n",
    "- [HLATR: Enhance Multi-stage Text Retrieval with Hybrid List Aware Transformer Reranking](https://arxiv.org/pdf/2205.10569.pdf)\n",
    "- [Condenser: a Pre-training Architecture for Dense Retrieval](https://arxiv.org/pdf/2104.08253.pdf)\n",
    "- [Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval](https://arxiv.org/pdf/2108.05540.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Odpowiadanie na pytania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mputo/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-11-30 13:41:04.407541: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-30 13:41:04.407580: I tensorflow/tsl/cuda/cudart_stub.cc:28] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-30 13:41:07.055208: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libnvinfer.so.8'; dlerror: libnvinfer.so.8: cannot open shared object file: No such file or directory\n",
      "2022-11-30 13:41:07.055376: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libnvinfer_plugin.so.8'; dlerror: libnvinfer_plugin.so.8: cannot open shared object file: No such file or directory\n",
      "2022-11-30 13:41:07.055387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MobileBertConfig, MobileBertModel\n",
    "\n",
    "# Initializing a MobileBERT configuration\n",
    "configuration = MobileBertConfig()\n",
    "\n",
    "# Initializing a model (with random weights) from the configuration above\n",
    "model = MobileBertModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "honolulu, hawaii\n",
      "2009\n",
      "columbia university\n",
      "2009 to 2017\n",
      "joe biden\n",
      "john mccain\n"
     ]
    }
   ],
   "source": [
    "from transformers import MobileBertTokenizer, MobileBertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = MobileBertTokenizer.from_pretrained(\"csarron/mobilebert-uncased-squad-v2\")\n",
    "model = MobileBertForQuestionAnswering.from_pretrained(\"csarron/mobilebert-uncased-squad-v2\")\n",
    "\n",
    "text = \"Barack Hussein Obama II (/bəˈrɑːk huːˈseɪn oʊˈbɑːmə/ (listen) bə-RAHK hoo-SAYN oh-BAH-mə;[1] born August 4, 1961) is an American politician who served as the 44th president of the United States from 2009 to 2017. A member of the Democratic Party, he was the first African-American president of the United States.[2] Obama previously served as a U.S. senator from Illinois from 2005 to 2008 and as an Illinois state senator from 1997 to 2004.\\\n",
    "Obama was born in Honolulu, Hawaii. After graduating from Columbia University in 1983, he worked as a community organizer in Chicago. In 1988, he enrolled in Harvard Law School, where he was the first black president of the Harvard Law Review. After graduating, he became a civil rights attorney and an academic, teaching constitutional law at the University of Chicago Law School from 1992 to 2004. Turning to elective politics, he represented the 13th district in the Illinois Senate from 1997 until 2004, when he ran for the U.S. Senate. Obama received national attention in 2004 with his March Senate primary win, his well-received July Democratic National Convention keynote address, and his landslide November election to the Senate. In 2008, after a close primary campaign against Hillary Clinton, he was nominated by the Democratic Party for president. Obama was elected over Republican nominee John McCain in the general election and was inaugurated alongside his running mate Joe Biden, on January 20, 2009. Nine months later, he was named the 2009 Nobel Peace Prize laureate, a decision that drew a mixture of praise and criticism.\"\n",
    "\n",
    "questions = [\"Where was Barack Obama born?\",\n",
    "            \"In what year did Barack Obama become a president?\",\n",
    "            \"What university did Obama attend?\",\n",
    "            \"In what time-period did Obama serve as president?\",\n",
    "            \"With whom did Barack Obama win presidential election?\",\n",
    "            \"Who did Obama beat in presidential election?\"]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    print(tokenizer.decode(predict_answer_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pytania kontrolne\n",
    "\n",
    "1. Jakie inne typy wyszukiwań są dostępne w wyszukiwarkach internetowych, oprócz wyszukiwania słów kluczowych?\n",
    "2. Jakie są trzy podstawowe modele wyszukiwania informacji?\n",
    "3. W oparciu o jakie technologie buduje się nowoczesne systemy wyszukiwania informacji?\n",
    "\n",
    "\n",
    "# Pytania do mnie? 🤔"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
