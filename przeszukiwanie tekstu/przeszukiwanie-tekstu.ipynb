{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Przeszukiwanie tekstu - algorytmy wyszukiwarek internetowych\n",
    "\n",
    "#### prezentuje Mateusz Puto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 1. Przeszukiwanie plik√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def bold(string):\n",
    "    return \"\\033[1m\" + string + \"\\033[0;0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Äî Dla pana ‚Äî odpowiedzia≈Ç dow√≥dca ‚Äî jestem kapitanem Nemo; pan za≈õ i pa≈Ñscy towarzysze jeste≈õcie dla mnie podr√≥≈ºnymi na ‚ÄûNautilusie‚Äù.\n",
      "\n",
      "\u001b[1m564\u001b[0;0m\n",
      "\n",
      "\u001b[1mPo przeczytaniu ksiƒÖ≈ºki pamiƒôtamy to: \n",
      "\u001b[0;0m‚Äî Dla pana ‚Äî odpowiedzia≈Ç dow√≥dca ‚Äî jestem kapitanem Nemo; pan za≈õ i pa≈Ñscy towarzysze jeste≈õcie dla mnie podr√≥≈ºnymi na ‚ÄûNautilusie‚Äù.\n",
      "\u001b[1m i to [spoiler !]:\n",
      "\u001b[0;0mSpodziewam siƒô, ≈ºe tak bƒôdzie. Mam nadziejƒô, ≈ºe potƒô≈ºny ‚ÄûNautilus‚Äù zwyciƒô≈ºy≈Ç morze w najstraszniejszej jego otch≈Çani, ≈ºe przetrwa≈Ç to, czego ≈ºaden okrƒôt przetrwaƒá nie zdo≈Ça≈Ç. Je≈õli tak jest, je≈õli kapitan Nemo ≈ºegluje jeszcze po morzach, kt√≥re mu sƒÖ przybranƒÖ ojczyznƒÖ, oby uczucie zemsty wygas≈Ço w dzikim jego sercu! Oby jƒÖ wygoni≈Ço z jego my≈õli rozglƒÖdanie siƒô w tylu cudach! Wykonawca sprawiedliwo≈õci niech ustƒÖpi miejsca uczonemu dokonywajƒÖcemu spokojnych poszukiwa≈Ñ podmorskich. Je≈õli los jego jest niezwyk≈Çy, to i wspania≈Çy zarazem. Sam to osobi≈õcie pojƒÖ≈Çem podczas dziesiƒôciomiesiƒôcznego, nadnaturalnego prawie mego istnienia.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"https://czytac.com/java-book/book/20_000_mil_podmorskiej_zeglugi\"\"\"\n",
    "\n",
    "# Wyszukiwanie samozaka≈ÑczajƒÖce\n",
    "dwa_tysiace_mil_podmorskiej_zeglugi = open('20_000_mil_podmorskiej_zeglugi.txt', 'r')\n",
    "\n",
    "for sentence in dwa_tysiace_mil_podmorskiej_zeglugi:\n",
    "    if \"Nautilus\" in sentence:\n",
    "        print(sentence)\n",
    "        break\n",
    "\n",
    "# Wyszukiwanie pe≈Çne\n",
    "dwa_tysiace_mil_podmorskiej_zeglugi = open('20_000_mil_podmorskiej_zeglugi.txt', 'r')\n",
    "\n",
    "nautilus_mentions = []\n",
    "for sentence in dwa_tysiace_mil_podmorskiej_zeglugi:\n",
    "    if \"Nautilus\" in sentence:\n",
    "        nautilus_mentions.append(sentence)\n",
    "\n",
    "print(bold(str(len(nautilus_mentions))), end=\"\\n\\n\")\n",
    "\n",
    "# Efekt niedawno≈õci (ang. \"recency effect\")\n",
    "print(bold(\"Po przeczytaniu ksiƒÖ≈ºki pamiƒôtamy to: \\n\") + nautilus_mentions[0] + bold(\" i to [spoiler !]:\\n\") + nautilus_mentions[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Miary jako≈õci - dok≈Çadno≈õƒá, precyzja i czu≈Ço≈õƒá](https://www.mimuw.edu.pl/~wjaworski/SU/SU04_miary_jakosci.pdf)\n",
    "\n",
    "\n",
    "Macierz b≈Çƒôd√≥w (ang. \"Confusion matrix\"):\n",
    "\n",
    "                                                                  Przewidywanie\n",
    "\n",
    "                                                         Dopasowanie         Brak dopasowania\n",
    "\n",
    "                                ---------------------------------------------------------------------\n",
    "\n",
    "                                Dopasowanie             True Positive        False Positive\n",
    "        Stan rzeczywisty                         \n",
    "                                          \n",
    "                                Brak dopasowania        False Negative       True Negative\n",
    "\n",
    "<br><br>\n",
    "                        \n",
    "(1.1) Dok≈Çadno≈õƒá = (TP + TN) / (TP + TN + FP + FN) <---- Poprawna klasyfikacja\n",
    "\n",
    "(1.2) Precyzja = TP / (TP + FN)  <---------------------------------- Ilo≈õƒá poprawnie znalezionych wynik√≥w spo≈õr√≥d tych wy≈õwietlonych.\n",
    "\n",
    "(1.3) Czu≈Ço≈õƒá = TP / (TP + FP)   <----------------------------------- Ilo≈õƒá znalezionych wynik√≥w spo≈õr√≥d wszystkich pasujƒÖcych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precyzja niepe≈Çnego wyszukania: 100.0% \n",
      "Czu≈Ço≈õƒá niepe≈Çnego wyszukania: 50.13%\n",
      "\n",
      "Precyzja niepoprawnego wyszukania: 51.07% \n",
      "Czu≈Ço≈õƒá niepoprawnego wyszukania: 100.0%\n"
     ]
    }
   ],
   "source": [
    "dwa_tysiace_mil_podmorskiej_zeglugi = open('20_000_mil_podmorskiej_zeglugi.txt', 'r')\n",
    "\n",
    "nemo = 0\n",
    "kapitan_nemo = 0\n",
    "nemo_or_ned = 0\n",
    "\n",
    "for sentence in dwa_tysiace_mil_podmorskiej_zeglugi:\n",
    "    # Poprawne wyszukanie\n",
    "    if \"Nemo\" in sentence:\n",
    "        nemo += 1\n",
    "    \n",
    "    # Wyszukanie, kt√≥re pomija czƒô≈õƒá wynik√≥w\n",
    "    if \"kapitan Nemo\" in sentence:\n",
    "        kapitan_nemo += 1\n",
    "\n",
    "    # Wyszukanie b≈Çƒôdne\n",
    "    if \"Nemo\" in sentence or \"Ned\" in sentence:\n",
    "        nemo_or_ned += 1\n",
    "\n",
    "print(f\"Precyzja niepe≈Çnego wyszukania: {round(100 * kapitan_nemo / kapitan_nemo, 2)}% \\nCzu≈Ço≈õƒá niepe≈Çnego wyszukania: {round(100 * kapitan_nemo / nemo, 2)}%\")\n",
    "print()\n",
    "print(f\"Precyzja niepoprawnego wyszukania: {round(100 * nemo / nemo_or_ned, 2)}% \\nCzu≈Ço≈õƒá niepoprawnego wyszukania: {round(100 * nemo / nemo, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### algorytm Rabina-Karpa\n",
    "\n",
    "Wersja uproszczona bez rolowania hash'a, wyszukiwania wielu wzorc√≥w.\n",
    "\n",
    "[Rabin-Karp algorithm Wikipedia](https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search with Rabin-Karp: 0.11455478300194954\n"
     ]
    }
   ],
   "source": [
    "from timeit import Timer\n",
    "from functools import partial\n",
    "\n",
    "def rabinKarp(pattern, text):\n",
    "    \"\"\"Simplified Rabin-Karp algorithm\"\"\"\n",
    "    \n",
    "    m = len(pattern)\n",
    "    n = len(text)\n",
    "\n",
    "    hpattern = hash(pattern)\n",
    "\n",
    "    for i in range(0, n-m+1):\n",
    "        htext = hash(text[i : i+m])\n",
    "        \n",
    "        if htext == hpattern:\n",
    "            if text[i :i+m] == pattern:\n",
    "                return i\n",
    "    else:\n",
    "        return \"Not found\"\n",
    "\n",
    "\n",
    "\n",
    "text = open('20_000_mil_podmorskiej_zeglugi.txt', 'r').read()\n",
    "pattern = \"pod≈Çug prawa natury wszystko idzie od prawej do lewej strony\"\n",
    "\n",
    "args = (pattern, text)\n",
    "t = Timer(partial(rabinKarp, *args))\n",
    "print(\"Search with Rabin-Karp:\", t.timeit(number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Zauwa≈ºmy, ≈ºe naiwne podej≈õce jest dla tego problemu znaczƒÖco szybsze. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive search: 0.09652286599884974\n"
     ]
    }
   ],
   "source": [
    "def naiveSearch(pattern, text):\n",
    "\n",
    "    m = len(pattern)\n",
    "    n = len(text)\n",
    "    \n",
    "    for i in range(0, n-m+1):\n",
    "        if text[i :i+m] == pattern:\n",
    "            return i\n",
    "    else:\n",
    "        return \"Not found\"\n",
    "\n",
    "text = open('20_000_mil_podmorskiej_zeglugi.txt', 'r').read()\n",
    "pattern = \"pod≈Çug prawa natury wszystko idzie od prawej do lewej strony\"\n",
    "\n",
    "args = (pattern, text)\n",
    "t = Timer(partial(naiveSearch, *args))\n",
    "print(\"Naive search:\", t.timeit(number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Jednak generalna koncepcja algorytmu mo≈ºe byƒá zastosowana do stworzenia systemu wyszukiwania informacji. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def precomputed_hashes(hpattern, hashes):        \n",
    "    return hpattern in hashes\n",
    "\n",
    "\n",
    "text = open('bible.txt', 'r')\n",
    "\n",
    "hashes = dict()\n",
    "verse = \"\"\n",
    "for paragraph in text:\n",
    "    if paragraph.strip() == \"\":\n",
    "        verse = verse.strip()\n",
    "\n",
    "        b = bytes(verse, encoding='utf-8')\n",
    "        hash_val = hashlib.sha256(b).hexdigest()\n",
    "        bucket = int(hash_val, 16) % 100\n",
    "\n",
    "        if bucket in hashes.keys():\n",
    "            hashes[bucket].append(hash_val)\n",
    "        else:\n",
    "            hashes[bucket] = list()\n",
    "            hashes[bucket].append(hash_val) \n",
    "\n",
    "        verse = \"\"\n",
    "    else:\n",
    "        verse += \" \" + paragraph.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple patterns hashed search: 0.0002994339956785552\n"
     ]
    }
   ],
   "source": [
    "not_in_text_patterns = [\"Oczekiwa≈Çem na kapitana Nemo, ale siƒô wcale nie pokaza≈Ç\",\n",
    "\"Zakomunikowa≈Çem te spostrze≈ºenia i obawy kapitanowi Nemo\",\n",
    "\"Poczekawszy jeszcze trochƒô, uda≈Çem siƒô do salonu\",\n",
    "\"≈ör√≥d takich okoliczno≈õci p≈Çynƒôli≈õmy a≈º do 13 marca\",\n",
    "\"Zak≈Çadanie dokonywa≈Ço siƒô pomy≈õlnie\",]\n",
    "\n",
    "bible_patterns = [\"1:3 And God said, Let there be light: and there was light.\", \n",
    "\"4:13 Therefore set I in the lower places behind the wall, and on the higher places, I even set the people after their families with their swords, their spears, and their bows.\",\n",
    "\"37:11 But the meek shall inherit the earth; and shall delight themselves in the abundance of peace.\",\n",
    "\"16:6 We have heard of the pride of Moab; he is very proud: even of his haughtiness, and his pride, and his wrath: but his lies shall not be so.\",\n",
    "\"5:8 And there are three that bear witness in earth, the Spirit, and the water, and the blood: and these three agree in one.\"]\n",
    "\n",
    "patterns = bible_patterns + not_in_text_patterns\n",
    "\n",
    "def findPatterns(patterns, hashes):\n",
    "    hpatterns = [hashlib.sha256(bytes(pattern, encoding='utf-8')).hexdigest() for pattern in patterns]\n",
    "\n",
    "    for hpattern in hpatterns:\n",
    "        bucket = int(hpattern, 16) % 100\n",
    "\n",
    "        if hpattern in hashes[bucket]:\n",
    "            pass\n",
    "            #print(\"Match\")\n",
    "        else:\n",
    "            pass\n",
    "            #print(\"Miss\")\n",
    "\n",
    "args = (patterns, hashes)\n",
    "t = Timer(partial(findPatterns, *args))\n",
    "print(\"Multiple patterns hashed search:\", t.timeit(number=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive search with multiple patterns: 0.06325988099706592\n"
     ]
    }
   ],
   "source": [
    "text = open('bible.txt', 'r')\n",
    "\n",
    "def naiveWholeSents(patterns, text):\n",
    "    verse = \"\"\n",
    "    for paragraph in text:\n",
    "        if paragraph.strip() == \"\":\n",
    "            verse = verse.strip()\n",
    "\n",
    "            if verse in patterns:\n",
    "                #print(\"Match\")\n",
    "                pass\n",
    "            \n",
    "            verse = \"\"\n",
    "        else:\n",
    "            verse += \" \" + paragraph.strip()\n",
    "        \n",
    "\n",
    "args = (patterns, text)\n",
    "t = Timer(partial(naiveWholeSents, *args))\n",
    "print(\"Naive search with multiple patterns:\", t.timeit(number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Wyszukiwanie pe≈Çnotekstowe\n",
    "\n",
    "- wykorzystywanie tylko tytu≈Ç√≥w i meta-tag√≥w\n",
    "- spamdexowanie\n",
    "- Kt√≥ra z wyszukiwarek internetowych by≈Ça jako pierwsza wyszukiwarkƒÖ pe≈ÇnotekstowƒÖ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"altavista.png\" alt=\"AltaVista search engine\" width=\"1000\"/>\n",
    "\n",
    "[ ≈∫r√≥d≈Ço: [https://digital.com/wp-content/uploads/4307188078_f290aecf49_o.png](https://digital.com/wp-content/uploads/4307188078_f290aecf49_o.png)]\n",
    "\n",
    "_______________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Indeksy w ksiƒÖ≈ºkach:\n",
    "\n",
    "<img src=\"book-index.jpg\" alt=\"Indexes in books\" />\n",
    "\n",
    "[ ≈∫r√≥d≈Ço: [https://book-editing.com/why-book-indexing/](https://www.book-editing.com/why-book-indexing/) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Odwr√≥cony indeks\n",
    "\n",
    "- budowanie indeksu\n",
    "- sortowanie\n",
    "- wyszukiwanie\n",
    "- dodatkowe informacje: czƒôsto≈õƒá, miejsce wystƒÖpienia\n",
    "- kompresja\n",
    "\n",
    "<img src=\"inverted-index.png\" width=400 />\n",
    "\n",
    "[ ≈∫r√≥d≈Ço: https://nlp.stanford.edu/IR-book/html/htmledition/a-first-take-at-building-an-inverted-index-1.html ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wyszukiwanie pe≈Çnotekstowe mo≈ºe s≈Çu≈ºyƒá poprawie nawigacji na stronie. Gdy widzimy okienko wyszukiwania na stronie jaka technologia zasila to rozwiƒÖzanie?\n",
    "\n",
    "\n",
    "- Apache Lucene i rozwiƒÖzania oparte o Lucene takie jak np. Solr- otwarto≈∫r√≥d≈Çowa biblioteka, najbardziej modyfikowalna opcja\n",
    "\n",
    "- Google Programmable Search Engine, AWS CloudSearch, Elasticsearch , Algolia itp. - a wiƒôc rozwiƒÖzania od r√≥≈ºnych dostawc√≥w takich us≈Çug\n",
    "\n",
    "- Wyszukiwanie bezpo≈õrednio w bazie danych lub rozwiƒÖzanie typu client-side (Czy naprawdƒô tego chcemy?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "----------------------------\n",
    "* [How to add full text search to your website by Sam Dutton](https://medium.com/dev-channel/how-to-add-full-text-search-to-your-website-4e9c80ce2bf4)\n",
    "<br><br>\n",
    "* [Apache Lucene](https://lucene.apache.org/)\n",
    "* [Lucene talk with tamar Syn-Hershko](https://www.youtube.com/watch?v=Nf9p-d01p78)\n",
    "<br><br>\n",
    "* [Google Programmable Search Engine](https://developers.google.com/custom-search)\n",
    "* [AWS CloudSearch](https://aws.amazon.com/cloudsearch/)\n",
    "* [Elasticsearch](https://www.elastic.co/)\n",
    "<br><br>\n",
    "* [Algolia vs Elasticsearch according to Algolia Part 1](https://www.algolia.com/blog/engineering/algolia-v-elasticsearch-latency/)\n",
    "* [Algolia vs Elasticsearch according to Algolia Part 2](https://www.algolia.com/blog/engineering/algolia-v-elasticsearch-relevance/)\n",
    "* [Inside the Algolia Engine Blog Series](https://www.algolia.com/blog/engineering/inside-the-algolia-engine-part-1-indexing-vs-search/)\n",
    "* [Algolia ranking algorithm](https://www.algolia.com/blog/how-algolia-tackled-the-relevance-problem-of-search-engines/)\n",
    "* [Algolia AI: vectors vs hashes](https://www.algolia.com/blog/ai/vectors-vs-hashes/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### przyk≈Çad Algolii\n",
    "\n",
    "<img src=\"algolia.svg\" width=500/>\n",
    "\n",
    "* Wyszukiwanie przyrostowe - wykorzystanie drzewa trie\n",
    "* Poprawianie b≈Çƒôd√≥w - dystans edycyjny Damerau-Levenshtein'a\n",
    "* Podkre≈õlanie znalezionych s≈Ç√≥w kluczowych\n",
    "* Lematyzacja i rozszerzanie zapytania\n",
    "* Tworzenie rankingu - algorytm tie-breaking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Wyszukiwanie informacji (IR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Rozszerzanie zapytania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meaning: a motor vehicle with four wheels; usually propelled by an internal combustion engine,\n",
      " lemmas: [Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'), Lemma('car.n.01.machine'), Lemma('car.n.01.motorcar')]\n",
      "\n",
      "meaning: a wheeled vehicle adapted to the rails of railroad,\n",
      " lemmas: [Lemma('car.n.02.car'), Lemma('car.n.02.railcar'), Lemma('car.n.02.railway_car'), Lemma('car.n.02.railroad_car')]\n",
      "\n",
      "meaning: the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant,\n",
      " lemmas: [Lemma('car.n.03.car'), Lemma('car.n.03.gondola')]\n",
      "\n",
      "meaning: where passengers ride up and down,\n",
      " lemmas: [Lemma('car.n.04.car'), Lemma('car.n.04.elevator_car')]\n",
      "\n",
      "meaning: a conveyance for passengers or freight on a cable railway,\n",
      " lemmas: [Lemma('cable_car.n.01.cable_car'), Lemma('cable_car.n.01.car')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "for syn in wordnet.synsets(\"car\"):\n",
    "    print(f\"meaning: {syn.definition()},\\n lemmas: {syn.lemmas()}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'car auto automobile machine motorcar'\n"
     ]
    }
   ],
   "source": [
    "def query_expansion(word, meaning=0):\n",
    "    query = ''\n",
    "    lemmas = wordnet.synsets(word)[meaning].lemmas()\n",
    "    for lemma in lemmas:\n",
    "\n",
    "        query += \" \" + lemma.name()\n",
    "\n",
    "    return query.strip()\n",
    "\n",
    "\n",
    "new_query = query_expansion(\"car\")\n",
    "print(f\"Query: '{new_query}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tokenizery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', '(', 'York', ')', '.']\n",
      "['Please', '(', 'buy', ')', 'me', 'two', 'of', 'them', '.']\n",
      "['(', 'Thanks', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, WhitespaceTokenizer\n",
    "\n",
    "sents = '''Good muffins cost $3.88\\nin New (York).  Please (buy) me\\ntwo of them.\\n(Thanks).'''\n",
    "\n",
    "tokens = []\n",
    "for sent in sent_tokenize(sents):\n",
    "    tokens.append(word_tokenize(sent))\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'i', 'onli', 'could', ',', 'i', \"'d\", 'be', 'run', 'up', 'that', 'hill']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "\n",
    "sent = '''If I only could, I'd be running up that hill'''\n",
    "\n",
    "tokens = []\n",
    "for token in word_tokenize(sent):\n",
    "    tokens.append(token)\n",
    "\n",
    "print([snowball.stem(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Model boolowski\n",
    "\n",
    "* Zazwyczaj ka≈ºde s≈Çowo jest interpretowane jako oddzielny token.\n",
    "* Wyszukiwane wyra≈ºenia mogƒÖ byƒá po≈ÇƒÖczone operatorami logicznymi AND, OR, a nawet NEAR.\n",
    "* Konieczne stworzenie odwr√≥conego indeksu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "from unittest import result\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class FullTextSearch:\n",
    "    \"\"\"Full-text search minimal example\"\"\"\n",
    "    def __init__(self):\n",
    "        self.inverted_index = {}\n",
    "        self.stop_words = stopwords.words()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.documents = {}\n",
    "\n",
    "\n",
    "    def search(self, query):\n",
    "        tokens = self.tokenize(query)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for token in tokens:\n",
    "            matches = set()\n",
    "\n",
    "            if token in self.inverted_index:\n",
    "                for document in self.inverted_index[token]:\n",
    "                    matches.add(document)\n",
    "            \n",
    "            results.append(matches)\n",
    "\n",
    "        return reduce(lambda x, y: x & y, results)\n",
    "\n",
    "    def add_document(self, doc, id):\n",
    "        self.documents[id]  = doc\n",
    "        self.index(doc, id)\n",
    "\n",
    "    def tokenize(self, query):\n",
    "        return word_tokenize(query)\n",
    "\n",
    "    def index(self, doc, id):\n",
    "        indexes = set()\n",
    "\n",
    "        for word in self.tokenize(doc):\n",
    "            if word not in self.stop_words:\n",
    "                indexes.add(self.stemmer.stem(word))\n",
    "\n",
    "        for word in indexes:\n",
    "            if not word in self.inverted_index:\n",
    "                self.inverted_index[word] = list()\n",
    "            self.inverted_index[word].append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "file_ids = movie_reviews.fileids()\n",
    "print(len(file_ids))\n",
    "\n",
    "fts_example = FullTextSearch()\n",
    "\n",
    "for id in file_ids[:100]:\n",
    "    fts_example.add_document(movie_reviews.raw(id), id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg/cv000_29416.txt'}\n",
      "{'neg/cv013_10494.txt', 'neg/cv026_29229.txt', 'neg/cv000_29416.txt'}\n"
     ]
    }
   ],
   "source": [
    "problem_review = fts_example.search(\"problem 4/10\")\n",
    "cool_kid_review = fts_example.search(\"cool kid\")\n",
    "\n",
    "print(problem_review, end=\"\\n\")\n",
    "print(cool_kid_review, end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Modele algebraiczne\n",
    "\n",
    "- bag-of-words, zlicza ilo≈õƒá wystƒÖpie≈Ñ (ang. \"term frequency\")\n",
    "- czƒôsto≈õƒá w dokumentach (ang. \"document frequency\")\n",
    "- idf (ang. \"inverse document frequency\")\n",
    "\n",
    "<img src=\"tf-idf.png\" />\n",
    "\n",
    "[ ≈∫r√≥d≈Ço: [https://pl.wikipedia.org/wiki/TFIDF](https://pl.wikipedia.org/wiki/TFIDF) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wykorzystanie **tf-idf**:\n",
    "\n",
    " - Model przestrzeni wektorowej\n",
    "\n",
    "<img src=\"vector-space.png\" />\n",
    "\n",
    "[ ≈∫r√≥d≈Ço: [https://www.researchgate.net/figure/A-query-and-document-representation-in-the-vector-space-model_fig1_220692395](https://www.researchgate.net/figure/A-query-and-document-representation-in-the-vector-space-model_fig1_220692395) ]\n",
    "\n",
    "Jak mierzyƒá podobie≈Ñstwo miƒôdzy dokumentami i zapytaniami?\n",
    "\n",
    "- kosinus miƒôdzy wektorami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Modele probabilistyczne\n",
    "\n",
    "- Probabilistyczna zasada szeregowania m√≥wi, ≈ºe dokumenty powinny byƒá uszeregowane w kolejno≈õci prawdopodobie≈Ñstwa ich u≈ºyteczno≈õci lub dopasowania. [ https://www.emerald.com/insight/content/doi/10.1108/eb026647/full/html?skipTracking=true ]\n",
    "\n",
    "- Dzia≈ÇajƒÖ w oparciu o zasadƒô¬†Bayesa (choƒá nie tylko: https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_People/Profs/goran/5-Probabilistic-Retrieval-FSS20.pdf)\n",
    "\n",
    "<img src=\"bayes.png\" />\n",
    "\n",
    "[ ≈∫r√≥d≈Ço: https://medium.com/@msalmon00/bayes-rule-and-cookies-a-primer-in-code-3ee15b6dc755 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- za≈Ço≈ºenie Binary Independence Model, nie wystƒôpujƒÖ zale≈ºno≈õci miƒôdzy elementami. Mo≈ºemy traktowaƒá ka≈ºdƒÖ czƒô≈õƒá wektora jako niezale≈ºnƒÖ jednostkƒô.\n",
    "\n",
    "- Najszerzej wykorzystywanym modelem probabilistycznym jest __BM25__.\n",
    "\n",
    "<img src=\"bm25.png\" />\n",
    "\n",
    "[ ≈∫r√≥d≈Ço: [https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_People/Profs/goran/5-Probabilistic-Retrieval-FSS20.pdf](https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_People/Profs/goran/5-Probabilistic-Retrieval-FSS20.pdf) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Relevance feedback\n",
    "\n",
    "<img src=\"relevance-feedback.jpg\" width=400 />\n",
    "\n",
    "[ ≈∫r√≥d≈Ço: (https://studfile.net/html/2706/143/html_oTTGrBg9KM._waU/htmlconvd-t8XooN216x1.jpg)[https://studfile.net/html/2706/143/html_oTTGrBg9KM._waU/htmlconvd-t8XooN216x1.jpg] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- algorytm Rocchia\n",
    "- strona UX oceniania jako≈õci wynik√≥w\n",
    "- clickthrough data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Modele jƒôzykowe\n",
    "\n",
    "- Model jƒôzykowy generuje rozk≈Çad prawdopodobie≈Ñstwa nad s≈Çowami ze s≈Çownika. \n",
    "- Mo≈ºe byƒá zrealizowany jako automat sko≈Ñczony, z kt√≥rego mo≈ºemy odczytaƒá prawdopodobie≈Ñstwa sekwencji. \n",
    "- Wygenerowany dla konkretnego dokumentu model jƒôzykowy jest u≈ºywany do okre≈õlenia prawdopodobie≈Ñstwa warunkowego na otrzymanie zapytania pod warunkiem, ≈ºe zaszed≈Ç warunek wystƒÖpienia modela jƒôzykowego dla dokumentu. \n",
    "\n",
    "\n",
    "<img src=\"language-model.png\" width=500/>\n",
    "[ ≈∫r√≥d≈Ço: https://towardsdatascience.com/the-beginners-guide-to-language-models-aa47165b57f9 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Wyszukiwarki internetowe\n",
    "\n",
    "IstniejƒÖ¬†r√≥≈ºne rozwiƒÖzania pozwalajƒÖce na wyszukiwanie w sieci, w≈õr√≥d nich: rozwiƒÖzanie Microsoftu Bing ‚Äì wbudowane w wyszukiwarkƒô Edge, Yahoo ‚Äì posiadajƒÖce kilkunastoprocentowy udzia≈Ç¬†w rynku wyszukiwarek w Japonii, czy Baidu ze swoim dominujƒÖcym udzia≈Çem w Chinach . Jednak niekwestionowanym liderem na globalnym rynku wyszukiwania internetowego pozostaje Google ze swoim ponad 90% udzia≈Çem.\n",
    "\n",
    "<img src=\"yahoo.png\" width=300/>\n",
    "<img src=\"bing.png\" width=300/>\n",
    "<img src=\"duckduckgo.png\" width=300/>\n",
    "<img src=\"baidu.png\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Jak dzia≈Ça Google wed≈Çug Google'a?\n",
    "\n",
    "- [How search works](https://www.google.com/search/howsearchworks/)\n",
    "- [\"Szczeg√≥≈Çowy\" przewodnik po dzia≈Çaniu wyszukiwarki Google](https://developers.google.com/search/docs/fundamentals/how-search-works)\n",
    "- [Search On 2022](https://blog.google/products/search/search-on-2022-announcements/)\n",
    "\n",
    "Jak naprawdƒô dzia≈Ça Google? Najlepiej sprawdziƒá to samemu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.brainkart.com/article/Trends-in-Information-Retrieval_11614/\n",
      "https://credencepressltd.com/journal/uploads/archive/202015998633983597529394.pdf\n",
      "https://misc.library.ucsb.edu/untangle/lager.html\n",
      "https://www.researchgate.net/publication/318530538_Trends_and_Issues_in_Modern_Information_Retrieval\n",
      "https://www.igi-global.com/chapter/information-retrieval-models/198585\n",
      "https://www.nowpublishers.com/INR\n",
      "https://www.academia.edu/33978409/_Trends_and_issues_in_Modern_Information_Retrieval_\n",
      "https://link.springer.com/chapter/10.1007/978-981-15-5554-1_14\n",
      "https://www.slideshare.net/abhay.ratnaparkhi/latest-trends-inaiandinformationretrieval-autosaved\n",
      "https://ieeexplore.ieee.org/document/8759000\n",
      "https://pubmed.ncbi.nlm.nih.gov/5515231/\n"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "\n",
    "query = 'new trends in information retrieval'\n",
    "for url in search(query):\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PajƒÖki (crawlery) internetowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Kryteria:\n",
    "    \n",
    "- Pokrycie (ang. \"coverage\"), czyli udzia≈Ç witryn odwiedzonych w≈õr√≥d wszystkich witryn\n",
    "- ≈öwie≈ºo≈õƒá (ang. \"freshness\"), czyli miara m√≥wiƒÖca nam jak dawno witryny by≈Çy odwiedzane\n",
    "- Pe≈Çno≈õƒá (ang. \"completeness\"), czyli jaka czƒô≈õƒá zasob√≥w z danej strony zosta≈Ça przetworzona i zindeksowana\n",
    "\n",
    "Dodatkowo pajƒÖk internetowy powinien cechowaƒá siƒô¬†uprzejmo≈õciƒÖ. Uprzejmy robot powinien nie nadwyrƒô≈ºaƒá zasob√≥w strony i nie wysy≈Çaƒá wszystkich ≈ºƒÖda≈Ñ o zasoby w jednym momencie. PajƒÖk powinien respektowaƒá ograniczenia na niego na≈Ço≈ºone, kt√≥re sƒÖ umieszczone w pliku o nazwie 'robots.txt'.\n",
    "\n",
    "https://www.google.com/robots.txt\n",
    "\n",
    "\n",
    "<img src=\"robots.png\" alt=\"przyk≈Çadowy plik robots.txt\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frontier size after 2 iterations of crawling: 105\n"
     ]
    }
   ],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "# https://en.wikipedia.org/wiki/List_of_HTTP_header_fields#Request_fields\n",
    "headers = {\n",
    "    'User-Agent': 'My tiny bot 0.1',\n",
    "    'From': 'mateuszmputo@gmail.com'\n",
    "}\n",
    "\n",
    "seed = ['https://mateuszputo.github.io/contact.html']\n",
    "\n",
    "frontier = seed\n",
    "iterations = 2\n",
    "for i in range(0, iterations):\n",
    "    page = requests.get(frontier[0], headers=headers)\n",
    "    webpage = html.fromstring(page.content)\n",
    "\n",
    "    urls = webpage.xpath('//a/@href')\n",
    "    [frontier.append(url) for url in urls]\n",
    "    frontier.pop(0)\n",
    "\n",
    "print(f\"Frontier size after {iterations} iterations of crawling: {len(frontier)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Crawling stron typu RIA\n",
    "\n",
    "Wraz z wprowadzeniem do u≈ºytku stron, kt√≥re w swojej konstrukcji nie opierajƒÖ siƒô na hiper≈ÇƒÖczac miƒôdzy podstronami, lecz generujƒÖ DOM strony dynamicznie przy u≈ºyciu skrypt√≥w JavaScript, ciƒô≈ºko m√≥wiƒá o pajƒÖkach internetowych nie wspominajƒÖc o Deep-Web crawlingu.\n",
    "\n",
    "- Jak przej≈õƒá po stronie kt√≥ra zmienia siƒô dynamicznie na podstawie akcji u≈ºytkownika?\n",
    "\n",
    "Nowoczesny pajƒÖk uruchamia takƒÖ¬†Rich Internet Application na silniku JavaScript, sprawdza czy widzia≈Ç danƒÖ stronƒô za pomocƒÖ modu≈Çu DOM-Seen i je≈õli nie to przeprowadza ekstrakcjƒô wszystkich zdarze≈Ñ do uruchomienia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Pytanie: Czym jest wiƒôkszo≈õƒá zdarze≈Ñ JavaScript, kt√≥re pr√≥bujemy uruchomiƒá?\n",
    "\n",
    "<img src=\"deepweb-crawler.png\" />\n",
    "\n",
    "[ ≈∫r√≥d≈Ço: [https://link.springer.com/article/10.1007/s11280-018-0602-1](https://link.springer.com/article/10.1007/s11280-018-0602-1) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Zaawansowane Googlowanie:\n",
    "\n",
    "- Wyszukiwanie frazy\n",
    "- U≈ºycie operator√≥w logicznych AND, OR\n",
    "- Wyszukiwanie na danej stronie \"site:\"\n",
    "- Wersja cache strony \"cache:\"\n",
    "- wy≈ÇƒÖczenia ze znakiem minus\n",
    "- Filtrowanie wynik√≥w\n",
    "\n",
    "... ≈ºeby zostaƒá ü•∑ü•∑ü•∑ googlowania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### PageRank\n",
    "\n",
    "By≈Ço na zajƒôciach - nie ma sensu przytaczaƒá.\n",
    "\n",
    "Model PageRank mo≈ºna interpretowaƒá jako modelujƒÖcy zachowanie losowego b≈ÇƒÖdzenia po stronach przez przeciƒôtnego u≈ºytkownika.\n",
    "\n",
    "<img src=\"surfer.jpg\" width=\"400\"/>\n",
    "\n",
    "[ ≈∫r√≥d≈Ço: [https://www.surfertoday.com/surfing/how-many-surfers-are-there-in-the-world](https://www.surfertoday.com/surfing/how-many-surfers-are-there-in-the-world)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Z ciekawostek: \n",
    "\n",
    "- PageRank by≈Ç powodem stworzenia wyszukiwarki Google.\n",
    "- Lepszy predyktor liczby cytowa≈Ñ od samej znanej liczby cytowa≈Ñ.\n",
    "\n",
    "[ ≈∫r√≥d≈Ço: [The PageRank Citation Ranking: Bringing Order to the Web](http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### BigTable\n",
    "\n",
    "- NoSQL-owa baza danych. \n",
    "- Ma postaƒá wielowymiarowej posortowanej tablicy asocjacyjnej (mapy).\n",
    "- Indeksowana w oparciu o klucz rzƒôdu i kolumny, zapisywany znacznik czasu.\n",
    "- Dane przechowywane jako nieinterpretowalny ciƒÖg bajt√≥w.\n",
    "- Pozwala na przechowywanie wielu wersji pliku w jednym rekordzie.\n",
    "- Kompresja osiƒÖgajƒÖca wynik 10 do 1.\n",
    "\n",
    "<img src=\"bigtable-architecture.svg\" width=500/>\n",
    "\n",
    "[ ≈∫r√≥d≈Ço: [https://cloud.google.com/bigtable/img/bigtable-architecture.svg](https://cloud.google.com/bigtable/img/bigtable-architecture.svg) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Wprowadzone zmiany\n",
    "\n",
    "- Graf wiedzy Google\n",
    "- opcja filtrowania wynik√≥w\n",
    "- obs≈Çuga wielu jƒôzyk√≥w\n",
    "- aktualizacja zapewniajƒÖca ≈õwie≈ºo≈õƒá \"caffeine\"\n",
    "- wyszukiwanie mowƒÖ oraz obrazem\n",
    "- i wiele innych: https://en.wikipedia.org/wiki/Timeline_of_Google_Search\n",
    "\n",
    "<img src=\"isaac-newton.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Wykorzystanie uczenia maszynowego\n",
    "\n",
    "Wyszukiwanie internetowe wykorzystuje du≈ºe zasoby danych. Nie jest wiƒôc dziwnym, ≈ºe wiele z problem√≥w wyszukiwania mo≈ºna usprawniƒá za pomocƒÖ¬†rozwiƒÖza≈Ñ uczenia maszynowego i uczenia g≈Çƒôbokiego. To, co w Algolii by≈Ço rozwiƒÖzywane za pomocƒÖ drzewa, czyli poprawianie liter√≥wek, Google rozwiƒÖzuje za pomocƒÖ sieci neuronowej sk≈ÇadajƒÖcej siƒô z 680 milion√≥w parametr√≥w, kt√≥rƒÖ¬†udaje im siƒô uruchomiƒá w dwie milisekundy\n",
    "\n",
    "- RankBrain\n",
    "\n",
    "Pierwszym z system√≥w g≈Çƒôbokiego uczenia wykorzystywanym w Googlu by≈Ç RankBrain. Pomaga on w ≈ÇƒÖczeniu s≈Ç√≥w i koncept√≥w. Jak podaje Google: gdy wpiszemy zapytanie pytajƒÖce o \"nazwƒô konsumenta bƒôdƒÖcego na szczycie ≈Ça≈Ñcucha pokarmowego\" to algorytm RankBrain pomo≈ºe w odnalezieniu wynik√≥w zawierajƒÖcych wspomnienie o \"drapie≈ºniku szczytowym\".\n",
    "\n",
    "Co ciekawe gdy sprawdzimy to has≈Ço, wyszukiwanie dzia≈Ça ale na razie tylko w jƒôzyku angielskim.\n",
    "\n",
    "- neural matching\n",
    "\n",
    "Kolejnym wa≈ºnym rozwiƒÖzaniem by≈Ço rozwiƒÖzanie \"neural matching\" ulepszajƒÖce ≈ÇƒÖczenie ze sobƒÖ zapyta≈Ñ ze stronami [42]. Dzia≈Ça ono prawdopodobnie wykorzystujƒÖc rozszerzanie zapytania i/lub dokumentu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- BERT\n",
    "\n",
    "W 2019 powsta≈Ç wielki model jƒôzykowy BERT, kt√≥ry umo≈ºliwia statystyczne rozumienie tekstu . Umo≈ºliwia on prawdopodobnie ponowne szeregowanie odnalezionych wynik√≥w (ang. \"re-ranking\"), a wg. Googla pozwala te≈º¬†na niepomijanie wa≈ºnych s≈Çow, kt√≥re inaczej mog≈Çyby byƒá uznane za s≈Çowa ze stop listy (ang. \"stop words\").\n",
    "\n",
    "- MUM\n",
    "\n",
    "Jednak najwiƒôkszym do tej pory przedsiƒôwziƒôciem jest MUM czyli \"Multitask Unified Model\" pozwalajƒÖcy na rozumienie i generowanie tekstu w 75 jƒôzykach oraz rozwiƒÖzujƒÖcy wiele rodzaj√≥w zada≈Ñ. Model ten jest r√≥wnie≈º multi modalny, mo≈ºe zrozumieƒá opr√≥cz tekstu r√≥wnie≈º¬†obraz. Model ma byƒá poczƒÖtkowo zintegrowany z Google Lens, wyszukiwarkƒÖ¬†obrazem w telefonie.\n",
    "\n",
    "- [How AI powers great search results](https://blog.google/products/search/how-ai-powers-great-search-results/)\n",
    "- [The ABCs of spelling in Google Search](https://blog.google/products/search/abcs-spelling-google-search/)\n",
    "- [MUM: A new AI milestone for understanding information](https://blog.google/products/search/introducing-mum/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 5. Przetwarzanie jƒôzyka naturalnego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "KsiƒÖ≈ºka do bibilioteki NLP w Pythonie:\n",
    "\n",
    "    - [NLTK Book](https://www.nltk.org/book/)\n",
    "\n",
    "<img src=\"nltk.jpg\" width=300 />\n",
    "\n",
    "[ ≈∫r√≥d≈Ço: [https://static01.helion.com.pl/global/okladki/326x466/e_2gte.jpg](https://static01.helion.com.pl/global/okladki/326x466/e_2gte.jpg) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Wyszukiwanie informacji u≈ºywa klasycznie statystycznych miar jƒôzykowych do okre≈õlania charakterystyk dokument√≥w. Dziedzina przetwarzania jƒôzyka naturalnego (NLP) zajmuje siƒô wykonywaniem manipulacji na tek≈õcie naturalnym, czyli takim, jakim pos≈ÇugujƒÖ siƒô ludzie na co dzie≈Ñ w mowie i pi≈õmie, przy u≈ºyciu komputer√≥w. Od prostego zliczania wystƒÖpie≈Ñ i innych miar statystycznych po zrozumienie tekstu i generowanie odpowiedzi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Hipoteza dystrybucyjna\n",
    "\n",
    "S≈Çowa o podobnym znaczeniu wystƒôpujƒÖ w podobnych kontekstach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Dystans edycyjny\n",
    "\n",
    "Dystans edycyjny to miara podobie≈Ñstwa dw√≥ch ≈Ça≈Ñcuch√≥w tekstowych, kt√≥ra jest mierzona za pomocƒÖ liczby podstawowych operacji, kt√≥re nale≈ºy wykonaƒá, aby zamieniƒá jeden z ≈Ça≈Ñcuch√≥w na drugi. Jednym z popularniejszych metryk jest odleg≈Ço≈õƒá Levenshteina pozwalajƒÖca na operacjƒô usuniƒôcia, dodania i zamiany znaku w ≈Ça≈Ñcuchu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. Zastosowanie metod uczenia maszynowego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Zadanie wyszukiwania informacji ad-hoc polega na znalezieniu rankingu najbardziej podobnego do optymalnego opartego o prawdopodobie≈Ñstwo dopasowania. Funkcja szeregowania Q √ó D ‚Üí R oblicza wynik dopasowania dla pary dokument, zapytanie. Funkcja taka mo≈ºe byƒá skonstruowana w ten spos√≥b, ≈ºe najpierw oblicza reprezentacje dla dokument√≥w, zapyta≈Ñ, par dokument√≥w i zapyta≈Ñ, a na ko≈Ñcu agreguje te reprezentacje. Takie systemy sƒÖ systemami reprezentacyjnymi, w przeciwie≈Ñstwie do system√≥w interakcyjnych, polegajƒÖcych na aktywnym przetworzeniu zapytania wraz z dokumentem, aby otrzymaƒá wynik dopasowania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Word2vec, doc2vec\n",
    "\n",
    "- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "- [Distributed Representations of Sentences and Documents](https://arxiv.org/pdf/1405.4053.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec vector operations 'paris - france + italy':\n",
      "[('italy', 0.8724863529205322), ('brazil', 0.823341429233551), ('regina', 0.764968752861023), ('thailand', 0.7616036534309387), ('greet', 0.7495027184486389), ('perform', 0.743165910243988), ('demi', 0.7425128221511841), ('roma', 0.7410842180252075), ('milan', 0.7396031618118286), ('band', 0.7381407022476196)]\n",
      "Basic usage 'similar to word %pc%':\n",
      "[('tablet', 0.8791574835777283), ('iphone', 0.8717852830886841), ('windows', 0.867980420589447), ('ipod', 0.8656975030899048), ('internet', 0.8508771061897278), ('notebook', 0.8482425212860107), ('flash', 0.8476564884185791), ('ipad', 0.8319953083992004), ('auto', 0.8283662796020508), ('app', 0.826338529586792)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
    "\n",
    "rome_vec = glove_vectors.get_vector(\"paris\", norm=False) - glove_vectors.get_vector(\"france\", norm=False) + glove_vectors.get_vector(\"italy\", norm=False)\n",
    "\n",
    "print(\"Word2vec vector operations 'paris - france + italy':\")\n",
    "print(glove_vectors.most_similar(rome_vec))\n",
    "\n",
    "print(\"Basic usage 'similar to word %pc%':\")\n",
    "print(glove_vectors.most_similar(\"pc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Transformery\n",
    "\n",
    "Czy chodzi o co≈õ takiego?\n",
    "\n",
    "<img src=\"transformers.jpg\" />\n",
    "\n",
    "[ ≈∫r√≥d≈Ço: [https://netflix.com](https://netflix.com)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Transformery\n",
    "\n",
    "Raczej o co≈õ takiego:\n",
    "\n",
    "<img src=\"transformer.png\" />\n",
    "\n",
    "[ ≈∫r√≥d≈Ço: [https://i.stack.imgur.com/eAKQu.png](https://i.stack.imgur.com/eAKQu.png)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### MS MARCO\n",
    "\n",
    "- Jeden z najpopularniejszych zbior√≥w danych dla trenowania i testowania modeli wyszukiwania informacji.\n",
    "- Sk≈Çada siƒô on z ponad miliona zapyta≈Ñ pozyskanych z log√≥w wyszukiwarki Bing, ka≈ºde z odpowiedziƒÖ wygenerowanƒÖ przez cz≈Çowieka.\n",
    "- Ponadto zbi√≥r ten zawiera prawie 9 milion√≥w paragraf√≥w pozyskanych z ponad 3,5 miliona dokument√≥w. Ustƒôpy sƒÖ wybrane jako 10 odpowiedzi na pytanie ustalonych przy pomocy wyszukiwarki Bing.\n",
    "- Kilka ustandaryzowanych zada≈Ñ w tym ranking dokument√≥w oraz ranking paragraf√≥w.\n",
    "- Najlepsze wyniki osiƒÖgniƒôte dla zada≈Ñ dostƒôpne na stronie g≈Ç√≥wnej projektu.\n",
    "\n",
    "<img src=\"msmarco.png\" width=500/>\n",
    "\n",
    "[ https://microsoft.github.io/msmarco/ ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Modele\n",
    "\n",
    "- BM25 + BERT, osiƒÖgniƒôto poprawƒô¬†27% na miarze MRR@10.\n",
    "- Doc2query, rozszerzenie dokumentu o 10 pyta≈Ñ (top-k sampling)\n",
    "- ColBert (z faiss), dziƒôki architekturze bienkodera osiƒÖga lepszƒÖ z≈Ço≈ºono≈õƒá czasowƒÖ.  \n",
    "- HLART, efektywny trzeci stopie≈Ñ wyszukiwania\n",
    "- PROP, dostosowany trening na modelowaniu jƒôzykowym [2021 SOTA]\n",
    "- Condenser i coCondenser [SOTA 2022]\n",
    "    * pretrening\n",
    "    * unikniƒôcie rozproszonej reprezentacji\n",
    "    * dotrenowanie na zadaniu okre≈õlania odleg≈Ço≈õci miƒôdzy dokumentami\n",
    "\n",
    "Prace naukowe:\n",
    "- [Passage re-ranking with BERT](https://arxiv.org/pdf/1901.04085.pdf)\n",
    "- [Document Expansion by Query Prediction](https://arxiv.org/pdf/1904.08375.pdf)\n",
    "- [ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://arxiv.org/pdf/2004.12832.pdf)\n",
    "- [HLATR: Enhance Multi-stage Text Retrieval with Hybrid List Aware Transformer Reranking](https://arxiv.org/pdf/2205.10569.pdf)\n",
    "- [Condenser: a Pre-training Architecture for Dense Retrieval](https://arxiv.org/pdf/2104.08253.pdf)\n",
    "- [Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval](https://arxiv.org/pdf/2108.05540.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Odpowiadanie na pytania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mputo/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-11-30 13:41:04.407541: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-30 13:41:04.407580: I tensorflow/tsl/cuda/cudart_stub.cc:28] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-30 13:41:07.055208: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libnvinfer.so.8'; dlerror: libnvinfer.so.8: cannot open shared object file: No such file or directory\n",
      "2022-11-30 13:41:07.055376: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libnvinfer_plugin.so.8'; dlerror: libnvinfer_plugin.so.8: cannot open shared object file: No such file or directory\n",
      "2022-11-30 13:41:07.055387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MobileBertConfig, MobileBertModel\n",
    "\n",
    "# Initializing a MobileBERT configuration\n",
    "configuration = MobileBertConfig()\n",
    "\n",
    "# Initializing a model (with random weights) from the configuration above\n",
    "model = MobileBertModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "honolulu, hawaii\n",
      "2009\n",
      "columbia university\n",
      "2009 to 2017\n",
      "joe biden\n",
      "john mccain\n"
     ]
    }
   ],
   "source": [
    "from transformers import MobileBertTokenizer, MobileBertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = MobileBertTokenizer.from_pretrained(\"csarron/mobilebert-uncased-squad-v2\")\n",
    "model = MobileBertForQuestionAnswering.from_pretrained(\"csarron/mobilebert-uncased-squad-v2\")\n",
    "\n",
    "text = \"Barack Hussein Obama II (/b…ôÀàr…ëÀêk huÀêÀàse…™n o äÀàb…ëÀêm…ô/ (listen) b…ô-RAHK hoo-SAYN oh-BAH-m…ô;[1] born August 4, 1961) is an American politician who served as the 44th president of the United States from 2009 to 2017. A member of the Democratic Party, he was the first African-American president of the United States.[2] Obama previously served as a U.S. senator from Illinois from 2005 to 2008 and as an Illinois state senator from 1997 to 2004.\\\n",
    "Obama was born in Honolulu, Hawaii. After graduating from Columbia University in 1983, he worked as a community organizer in Chicago. In 1988, he enrolled in Harvard Law School, where he was the first black president of the Harvard Law Review. After graduating, he became a civil rights attorney and an academic, teaching constitutional law at the University of Chicago Law School from 1992 to 2004. Turning to elective politics, he represented the 13th district in the Illinois Senate from 1997 until 2004, when he ran for the U.S. Senate. Obama received national attention in 2004 with his March Senate primary win, his well-received July Democratic National Convention keynote address, and his landslide November election to the Senate. In 2008, after a close primary campaign against Hillary Clinton, he was nominated by the Democratic Party for president. Obama was elected over Republican nominee John McCain in the general election and was inaugurated alongside his running mate Joe Biden, on January 20, 2009. Nine months later, he was named the 2009 Nobel Peace Prize laureate, a decision that drew a mixture of praise and criticism.\"\n",
    "\n",
    "questions = [\"Where was Barack Obama born?\",\n",
    "            \"In what year did Barack Obama become a president?\",\n",
    "            \"What university did Obama attend?\",\n",
    "            \"In what time-period did Obama serve as president?\",\n",
    "            \"With whom did Barack Obama win presidential election?\",\n",
    "            \"Who did Obama beat in presidential election?\"]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    print(tokenizer.decode(predict_answer_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pytania kontrolne\n",
    "\n",
    "1. Jakie inne typy wyszukiwa≈Ñ sƒÖ dostƒôpne w wyszukiwarkach internetowych, opr√≥cz wyszukiwania s≈Ç√≥w kluczowych?\n",
    "2. Jakie sƒÖ trzy podstawowe modele wyszukiwania informacji?\n",
    "3. W oparciu o jakie technologie buduje siƒô nowoczesne systemy wyszukiwania informacji?\n",
    "\n",
    "\n",
    "# Pytania do mnie? ü§î"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
